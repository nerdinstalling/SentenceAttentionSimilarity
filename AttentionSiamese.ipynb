{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Layer\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "from IPython.display import SVG\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x,y = vects\n",
    "    sum_square = K.sum(K.square(x-y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "with open('./data/dwords.p', 'rb') as f:\n",
    "    dtr = pkl.load(f)\n",
    "\n",
    "\n",
    "def load_set(embed, datapath, embed_dim):\n",
    "    with open(datapath, 'rb') as f:\n",
    "        mylist = pkl.load(f)\n",
    "    s0 = []\n",
    "    s1 = []\n",
    "    labels = []\n",
    "    for each in mylist:\n",
    "        s0x = each[0]\n",
    "        s1x = each[1]\n",
    "        label = each[2]\n",
    "        score = float(label)\n",
    "        labels.append(score)\n",
    "        for i, ss in enumerate([s0x, s1x]):\n",
    "            words = word_tokenize(ss)\n",
    "            index = []\n",
    "            for word in words:\n",
    "                if word in dtr:\n",
    "                    index.append(embed[dtr[word]])\n",
    "                elif word in embed.vocab:\n",
    "                    index.append(embed[word])\n",
    "                else:\n",
    "                    index.append(np.zeros(embed_dim, dtype=float))\n",
    "\n",
    "            if i == 0:\n",
    "                s0.append(np.array(index, dtype=float))\n",
    "            else:\n",
    "                s1.append(np.array(index, dtype=float))\n",
    "\n",
    "    return [s0, s1, labels]\n",
    "\n",
    "\n",
    "\n",
    "def load_data(max_len, embed, datapath, embed_dim):\n",
    "    train_set = load_set(embed, datapath, embed_dim)\n",
    "    train_set_x1, train_set_x2, train_set_y = train_set\n",
    "    # train_set length\n",
    "    n_samples = len(train_set_x1)\n",
    "\n",
    "    \n",
    "    sidx = np.random.permutation(n_samples)\n",
    "\n",
    "    train_set_x1 = [train_set_x1[s] for s in sidx]\n",
    "    train_set_x2 = [train_set_x2[s] for s in sidx]\n",
    "    train_set_y = [train_set_y[s] for s in sidx]\n",
    "\n",
    "    \n",
    "    train_set = [train_set_x1, train_set_x2, train_set_y]\n",
    "\n",
    "    \n",
    "    new_train_set_x1 = np.zeros([len(train_set[0]), max_len, embed_dim], dtype=float)\n",
    "    new_train_set_x2 = np.zeros([len(train_set[0]), max_len, embed_dim], dtype=float)\n",
    "    new_train_set_y = np.zeros(len(train_set[0]), dtype=float)\n",
    "    mask_train_x1 = np.zeros([len(train_set[0]), max_len])\n",
    "    mask_train_x2 = np.zeros([len(train_set[0]), max_len])\n",
    "\n",
    "    def padding_and_generate_mask(x1, x2, y, new_x1, new_x2, new_y, new_mask_x1, new_mask_x2):\n",
    "\n",
    "        for i, (x1, x2, y) in enumerate(zip(x1, x2, y)):\n",
    "            # whether to remove sentences with length larger than maxlen\n",
    "            if len(x1) <= max_len:\n",
    "                new_x1[i, 0:len(x1)] = x1\n",
    "                new_mask_x1[i, len(x1) - 1] = 1\n",
    "                new_y[i] = y\n",
    "            else:\n",
    "                new_x1[i, :, :] = (x1[0:max_len:embed_dim])\n",
    "                new_mask_x1[i, max_len - 1] = 1\n",
    "                new_y[i] = y\n",
    "            if len(x2) <= max_len:\n",
    "                new_x2[i, 0:len(x2)] = x2\n",
    "                new_mask_x2[i, len(x2) - 1] = 1  \n",
    "                new_y[i] = y\n",
    "            else:\n",
    "                new_x2[i, :, :] = (x2[0:max_len:embed_dim])\n",
    "                new_mask_x2[i, max_len - 1] = 1\n",
    "                new_y[i] = y\n",
    "        new_set = [new_x1, new_x2, new_y, new_mask_x1, new_mask_x2]\n",
    "        del new_x1, new_x2, new_y\n",
    "        return new_set\n",
    "\n",
    "    train_set = padding_and_generate_mask(train_set[0], train_set[1], train_set[2], new_train_set_x1, new_train_set_x2,\n",
    "                                          new_train_set_y, mask_train_x1, mask_train_x2)\n",
    "\n",
    "    return train_set\n",
    "\n",
    "\n",
    "# return batch dataset\n",
    "def batch_iter(data, batch_size):\n",
    "    # get dataset and label\n",
    "    x1, x2, y, mask_x1, mask_x2 = data\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    y = np.array(y)\n",
    "    data_size = len(x1)\n",
    "    num_batches_per_epoch = int((data_size - 1) / batch_size) + 1\n",
    "    for batch_index in range(num_batches_per_epoch):\n",
    "        start_index = batch_index * batch_size\n",
    "        end_index = min((batch_index + 1) * batch_size, data_size)\n",
    "        return_x1 = x1[start_index:end_index]\n",
    "        return_x2 = x2[start_index:end_index]\n",
    "        return_y = y[start_index:end_index]\n",
    "        return_mask_x1 = mask_x1[start_index:end_index]\n",
    "        return_mask_x2 = mask_x2[start_index:end_index]\n",
    "\n",
    "        yield [return_x1, return_x2, return_y, return_mask_x1, return_mask_x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSeqAttn(nn.Module):\n",
    "    \"\"\"Self attention over a sequence:\n",
    "    * o_i = softmax(Wx_i) for x_i in X.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearSeqAttn, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"\n",
    "        x = batch * len * hdim\n",
    "        x_mask = batch * len\n",
    "        \"\"\"\n",
    "        x_flat = x.contiguous().view(-1, x.size(-1))\n",
    "        scores = self.linear(x_flat).view(x.size(0), x.size(1))\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-742b3f83b5e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBilinearSeqAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"A bilinear attention layer over a sequence X w.r.t y:\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m*\u001b[0m \u001b[0mo_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mWy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mOptionally\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BilinearSeqAttn(nn.Module):\n",
    "    \"\"\"A bilinear attention layer over a sequence X w.r.t y:\n",
    "    * o_i = softmax(x_i'Wy) for x_i in X.\n",
    "\n",
    "    Optionally don't normalize output weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_size, y_size, identity=False):\n",
    "        super(BilinearSeqAttn, self).__init__()\n",
    "        if not identity:\n",
    "            self.linear = nn.Linear(y_size, x_size)\n",
    "        else:\n",
    "            self.linear = None\n",
    "\n",
    "    def forward(self, x, y, x_mask):\n",
    "        \"\"\"\n",
    "        x = batch * len * h1\n",
    "        y = batch * h2\n",
    "        x_mask = batch * len\n",
    "        \"\"\"\n",
    "        Wy = self.linear(y) if self.linear is not None else y\n",
    "        xWy = x.bmm(Wy.unsqueeze(2)).squeeze(2)\n",
    "        xWy.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        if self.training:\n",
    "            # In training we output log-softmax for NLL\n",
    "            alpha = F.log_softmax(xWy, dim=1)\n",
    "        else:\n",
    "            # ...Otherwise 0-1 probabilities\n",
    "            alpha = F.softmax(xWy, dim=1)\n",
    "        return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1a2de5069d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_set' is not defined"
     ]
    }
   ],
   "source": [
    "abc = load_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
